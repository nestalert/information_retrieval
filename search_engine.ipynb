{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b226c97-fab6-406e-903f-b1603c31a537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose search type (boolean/bm25/vsm):  bm25\n",
      "Enter your query:  help\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filtered_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m     search(index, tfidf_data, query, search_type, df)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m     split_and_search(df, index, tfidf_data, query, search_type, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 72\u001b[0m, in \u001b[0;36msplit_and_search\u001b[1;34m(df, index, tfidf_data, query, search_type, num_splits)\u001b[0m\n\u001b[0;32m     69\u001b[0m         all_results\u001b[38;5;241m.\u001b[39mextend(sub_results)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Sort results by score\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(filtered_results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m sorted_results:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_results' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def search_tfidf(tfidf_data, article_id, query_word):\n",
    "    tfidf_values = []\n",
    "    for article in tfidf_data:\n",
    "        if article['title'] == article_id:\n",
    "            tfidf_value = article['tfidf'].get(query_word, 0.0)\n",
    "            tfidf_values.append(tfidf_value)\n",
    "    return tfidf_values\n",
    "\n",
    "def create_dataframe(articles):\n",
    "    df = pd.DataFrame(articles)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bm25_score(query_tfidf, doc_tfidf, k1=1.2, b=0.75):\n",
    "    doc_lengths = np.sum(doc_tfidf, axis=1)\n",
    "    avg_doc_length = np.mean(doc_lengths)\n",
    "\n",
    "    # Vectorized BM25 calculation\n",
    "    denom = k1 * (1 - b) + b * np.expand_dims(doc_lengths, axis=1) / avg_doc_length + doc_tfidf\n",
    "    scores = np.sum(query_tfidf * (k1 + 1) * doc_tfidf / denom, axis=1)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def vsm_search(query, tfidf_matrix, vectorizer, df):\n",
    "  # Transform the query using the existing vectorizer\n",
    "  query_vector = vectorizer.transform([query]).toarray().squeeze()\n",
    "\n",
    "  # Calculate cosine similarity\n",
    "  cosine_similarities = cosine_similarity(query_vector.reshape(1, -1), tfidf_matrix).flatten()\n",
    "\n",
    "  # Sort results\n",
    "  results = sorted(enumerate(cosine_similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  return [(df.iloc[idx]['title'], score) for idx, score in results]\n",
    "\n",
    "def split_and_search(df, index, tfidf_data, query, search_type, num_splits):\n",
    "    # Calculate document lengths\n",
    "    df['doc_len'] = df['content'].str.len()\n",
    "\n",
    "    # Sort by document length\n",
    "    df = df.sort_values(by='doc_len')\n",
    "\n",
    "    # Calculate chunk sizes\n",
    "    chunk_size = len(df) // num_splits\n",
    "\n",
    "    # Create a list to store the split DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Split the DataFrame into even chunks based on document lengths\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        df_list.append(df[i:i+chunk_size])\n",
    "\n",
    "    # Perform search on each chunk\n",
    "    all_results = []\n",
    "    for sub_df in df_list:\n",
    "        sub_results = search(index, tfidf_data, query, search_type, df=sub_df)\n",
    "        if sub_results:\n",
    "            all_results.extend(sub_results)\n",
    "\n",
    "    # Sort results by score\n",
    "    sorted_results = sorted(all_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print results\n",
    "    for result in sorted_results:\n",
    "        print(result)\n",
    "\n",
    "def search(index, tfidf_data, query, search_type, df=None):\n",
    "    if df is None:\n",
    "        df = create_dataframe(articles)\n",
    "    words = query.split()\n",
    "    result = set()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df['content'])\n",
    "    \n",
    "    if search_type == 'boolean':\n",
    "        if len(words) == 1:\n",
    "            # Single word query\n",
    "            result.update(index.get(words[0], set()))\n",
    "        else:\n",
    "            # Boolean query\n",
    "            set1 = set(index.get(words[0], set()))\n",
    "            set2 = set(index.get(words[2], set()))\n",
    "            operator = words[1]\n",
    "            if operator == 'AND':\n",
    "                result = set1 & set2\n",
    "            elif operator == 'OR':\n",
    "                result = set1 | set2\n",
    "            elif operator == 'NOT':\n",
    "                result = set1 - set2\n",
    "        results_with_tfidf = [(article_id, search_tfidf(tfidf_data, article_id, words[0])) for article_id in result]\n",
    "        sorted_results = sorted(results_with_tfidf, key=lambda x: max(x[1]), reverse=True)\n",
    "        for article_id, tfidf_values in sorted_results:\n",
    "            max_tfidf = max(tfidf_values)\n",
    "            print(f\"{article_id} ({max_tfidf})\")\n",
    "    elif search_type == 'bm25':\n",
    "        query_tfidf = vectorizer.transform([query]).toarray()[0]\n",
    "        bm25_scores = bm25_score(query_tfidf, X.toarray())\n",
    "        result = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)\n",
    "        return [(df.iloc[article_idx]['title'], score) for rank, (article_idx, score) in enumerate(result) if score > 0.0]\n",
    "    elif search_type == 'vsm':\n",
    "        return vsm_search(query, X, vectorizer, df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    index = load(\"index.json\")\n",
    "    tfidf_data = load(\"tf-idf.json\")\n",
    "    articles = load(\"articles.json\")\n",
    "    df = create_dataframe(articles)\n",
    "    search_type = input(\"Choose search type (boolean/bm25/vsm): \")\n",
    "    query = input(\"Enter your query: \")\n",
    "    if search_type == 'boolean':\n",
    "        search(index, tfidf_data, query, search_type, df)\n",
    "    else:\n",
    "        split_and_search(df, index, tfidf_data, query, search_type, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574dd8b-f02a-4530-bfd0-b414fbeb8142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc2f79-af14-46b7-8753-6273454ef8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
