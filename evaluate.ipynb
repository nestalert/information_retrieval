{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1ae58f-5df3-41bb-b38f-9251099e7aee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 128\u001b[0m\n\u001b[0;32m    125\u001b[0m search_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvsm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    127\u001b[0m documents, queries, relevant_docs \u001b[38;5;241m=\u001b[39m load_cisi_data(doc_file, query_file, rel_file)\n\u001b[1;32m--> 128\u001b[0m evaluate_cisi(documents, queries, relevant_docs, search_type)\n",
      "Cell \u001b[1;32mIn[3], line 101\u001b[0m, in \u001b[0;36mevaluate_cisi\u001b[1;34m(documents, queries, relevant_docs, search_type)\u001b[0m\n\u001b[0;32m     99\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m--> 101\u001b[0m     retrieved_docs \u001b[38;5;241m=\u001b[39m search_cisi(df, query[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], search_type)\n\u001b[0;32m    102\u001b[0m     retrieved_doc_ids \u001b[38;5;241m=\u001b[39m [doc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m retrieved_docs]\n\u001b[0;32m    104\u001b[0m     precision \u001b[38;5;241m=\u001b[39m calculate_precision(retrieved_doc_ids, relevant_docs[query[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[3], line 59\u001b[0m, in \u001b[0;36msearch_cisi\u001b[1;34m(df, query, search_type)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_cisi\u001b[39m(df, query, search_type):\n\u001b[0;32m     58\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 59\u001b[0m     X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m search_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvsm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     61\u001b[0m         query_vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([query])\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_cisi_data(doc_file, query_file, rel_file):\n",
    "    documents = []\n",
    "    with open(doc_file, 'r') as f:\n",
    "        doc_id = None\n",
    "        doc_text = \"\"\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\".I\"):\n",
    "                if doc_id:\n",
    "                    documents.append({\"id\": doc_id, \"text\": doc_text})\n",
    "                doc_id = line.split()[1]\n",
    "                doc_text = \"\"\n",
    "            elif line.startswith(\".W\"):\n",
    "                doc_text += line[3:] + \" \"\n",
    "        if doc_id:\n",
    "            documents.append({\"id\": doc_id, \"text\": doc_text})\n",
    "\n",
    "    queries = []\n",
    "    with open(query_file, 'r') as f:\n",
    "        query_id = None\n",
    "        query_text = \"\"\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\".I\"):\n",
    "                if query_id:\n",
    "                    queries.append({\"id\": query_id, \"text\": query_text})\n",
    "                query_id = line.split()[1]\n",
    "                query_text = \"\"\n",
    "            elif line.startswith(\".W\"):\n",
    "                query_text += line[3:] + \" \"\n",
    "        if query_id:\n",
    "            queries.append({\"id\": query_id, \"text\": query_text})\n",
    "\n",
    "    relevant_docs = {}\n",
    "    with open(rel_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                query_id = parts[0]\n",
    "                doc_id = parts[1] \n",
    "                if query_id not in relevant_docs:\n",
    "                    relevant_docs[query_id] = []\n",
    "                relevant_docs[query_id].append(doc_id)\n",
    "\n",
    "    return documents, queries, relevant_docs\n",
    "\n",
    "def create_dataframe(documents):\n",
    "    df = pd.DataFrame(documents)\n",
    "    return df\n",
    "\n",
    "def search_cisi(df, query, search_type):\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df['text'])\n",
    "    if search_type == 'vsm':\n",
    "        query_vector = vectorizer.transform([query]).toarray().squeeze()\n",
    "        cosine_similarities = cosine_similarity(query_vector.reshape(1, -1), X).flatten()\n",
    "        results = sorted(enumerate(cosine_similarities), key=lambda x: x[1], reverse=True)\n",
    "        return [(df.iloc[idx]['id'], score) for idx, score in results]\n",
    "\n",
    "def calculate_precision(retrieved_docs, relevant_docs):\n",
    "    if len(retrieved_docs) == 0:\n",
    "        return 0.0\n",
    "    true_positives = len(set(retrieved_docs) & set(relevant_docs))\n",
    "    return true_positives / len(retrieved_docs)\n",
    "\n",
    "def calculate_recall(retrieved_docs, relevant_docs):\n",
    "    if len(relevant_docs) == 0:\n",
    "        return 0.0\n",
    "    true_positives = len(set(retrieved_docs) & set(relevant_docs))\n",
    "    return true_positives / len(relevant_docs)\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision == 0 and recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    average_precisions = []\n",
    "    num_relevant = len(relevant_docs)\n",
    "    \n",
    "    for i, doc_id in enumerate(retrieved_docs):\n",
    "        if doc_id in relevant_docs:\n",
    "            precision = (i + 1) / (i + 1)\n",
    "            average_precisions.append(precision)\n",
    "    \n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return sum(average_precisions) / num_relevant\n",
    "\n",
    "def evaluate_cisi(documents, queries, relevant_docs, search_type):\n",
    "    df = create_dataframe(documents)\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        retrieved_docs = search_cisi(df, query['text'], search_type)\n",
    "        retrieved_doc_ids = [doc[0] for doc in retrieved_docs]\n",
    "        \n",
    "        precision = calculate_precision(retrieved_doc_ids, relevant_docs[query['id']])\n",
    "        recall = calculate_recall(retrieved_doc_ids, relevant_docs[query['id']])\n",
    "        f1_score = calculate_f1_score(precision, recall)\n",
    "        map_score = calculate_map(retrieved_doc_ids, relevant_docs[query['id']])\n",
    "        \n",
    "        results.append((precision, recall, f1_score, map_score))\n",
    "    \n",
    "    avg_precision = np.mean([result[0] for result in results])\n",
    "    avg_recall = np.mean([result[1] for result in results])\n",
    "    avg_f1_score = np.mean([result[2] for result in results])\n",
    "    avg_map = np.mean([result[3] for result in results])\n",
    "    \n",
    "    print(f\"Average Precision: {avg_precision}\")\n",
    "    print(f\"Average Recall: {avg_recall}\")\n",
    "    print(f\"Average F1-Score: {avg_f1_score}\")\n",
    "    print(f\"Mean Average Precision: {avg_map}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    doc_file = \"cisi.all\"\n",
    "    query_file = \"cisi.qry\"\n",
    "    rel_file = \"cisi.rel\"\n",
    "    search_type = 'vsm'\n",
    "\n",
    "    documents, queries, relevant_docs = load_cisi_data(doc_file, query_file, rel_file)\n",
    "    evaluate_cisi(documents, queries, relevant_docs, search_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05f565-eb93-4f23-a4d2-9dcbc46293f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
